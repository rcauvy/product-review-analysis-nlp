{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Sentiment Analysis - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student name: Robert Cauvy \n",
    "* Student pace: Flex\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Claude Fried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE OF CONTENTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click to jump to matching Markdown Header.*<br><br>\n",
    " \n",
    "- **[Introduction](#INTRODUCTION)<br>**\n",
    "- **[OBTAIN](#OBTAIN)**<br>\n",
    "- **[SCRUB](#SCRUB)**<br>\n",
    "- **[EXPLORE](#EXPLORE)**<br>\n",
    "- **[MODEL](#MODEL)**<br>\n",
    "- **[iNTERPRET](#iNTERPRET)**<br>\n",
    "- **[Conclusions/Recommendations](#CONCLUSIONS-&-RECOMMENDATIONS)<br>**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hired by Apple to determine which product release has the more positive sentiment and how it compared to their competitor Google who had also just released a new service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past decade conversations have increasingly shifted towards social media. Businesses across all industries could stand to benefit from listening to these conversations about themselves and how their products and brand are perceived by they users and prospective customers. Understanding what it is that customers enjoy the most and the least about your company's products and brand is crucial to retaining your loyal customers as well as attracting new ones.\n",
    "\n",
    "When large companies announce their new product releases at conferences and keynotes, they can obtain useful market insights and feedback from public opinion. A great source to measure market reactions is the giant social media network, Twitter.\n",
    "\n",
    "In addition to analyzing tweets various machine learning models will be trained and tested to classify tweets as either positive or negative sentiments towards the companies products and services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not easy to obtain unbiased and unfiltered feedback and opinions from the public. Understanding how the market feels about the products and services delivered by your brand in real-time can provide valuable insights that could not get captured before the ubiquity of social media. Applying human capital to track social networks is simply not a scalable solution which makes the application of Natural Language Processing and Machine Learning classifiers well suited for this business problem.\n",
    "\n",
    "The objective of this project is provide the businesses (Apple and Google) a model that identifies which tweets hold either a positive or negative sentiment about their brand or products from a corpus of tweets. Furthermore, this project will provide the stakeholders with a list of topics and keywords that most affect public perception, leaving actionable insights for future marketing and product design decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.179739Z",
     "start_time": "2022-02-25T20:25:56.354673Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,TfidfTransformer, \n",
    "                                             TfidfVectorizer,ENGLISH_STOP_WORDS)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import nltk \n",
    "from nltk import TweetTokenizer, word_tokenize,wordpunct_tokenize\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.201011Z",
     "start_time": "2022-02-25T20:26:04.188820Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is utilizing a dataset  provided by CrowdFlower to from data.world. The dataset contains over 9,000 tweets from SXSW(South by Southwest) Conference about new product releases from Apple and Google. The tweet have been labeled as to which emotion they convey towards a particular product category or company brand based off of the language contained in the tweet.\n",
    "\n",
    "According to the provider of the dataset, humans that were tasked with labeling the sentiments of each tweet by evaluating which brand or product the tweet was about and if the tweet expressed positive, negative, or no emotion towards a brand and/or product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.242955Z",
     "start_time": "2022-02-25T20:26:04.217826Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## NLP Imports\n",
    "import nltk\n",
    "from nltk import FreqDist,word_tokenize,regexp_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.394919Z",
     "start_time": "2022-02-25T20:26:04.261328Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tweet_product.csv', encoding= 'unicode_escape')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.419589Z",
     "start_time": "2022-02-25T20:26:04.405267Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.438132Z",
     "start_time": "2022-02-25T20:26:04.428357Z"
    }
   },
   "outputs": [],
   "source": [
    "# renaming columns to reduce verbosity\n",
    "df = df.rename(columns={\"tweet_text\": \"text\", \n",
    "                   \"emotion_in_tweet_is_directed_at\": \"product\",\n",
    "                  \"is_there_an_emotion_directed_at_a_brand_or_product\":\"sentiment\"\n",
    "                  }\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.477103Z",
     "start_time": "2022-02-25T20:26:04.446224Z"
    }
   },
   "outputs": [],
   "source": [
    "#Cleaning up the values in sentinemts for easier interpretability\n",
    "\n",
    "sentiment_dict = {'Positive emotion': 'Positive', 'Negative emotion': 'Negative', \n",
    "                'No emotion toward brand or product': 'Neutral', \n",
    "                \"I can't tell\": 'Unknown'}\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.554776Z",
     "start_time": "2022-02-25T20:26:04.491501Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.590584Z",
     "start_time": "2022-02-25T20:26:04.570622Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a variable \"corpus\" containing all text\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "corpus = df['text'].to_list()\n",
    "\n",
    "## Preview first 5 entries \n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.627058Z",
     "start_time": "2022-02-25T20:26:04.600133Z"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated(subset=['text'], keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.699164Z",
     "start_time": "2022-02-25T20:26:04.632813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at duplicated records\n",
    "duplicates = df.duplicated(subset=['text'], keep=False)\n",
    "df.loc[duplicates.loc[duplicates==True].index].sort_values(by='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.755407Z",
     "start_time": "2022-02-25T20:26:04.704351Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "# check for duplicates\n",
    "df.duplicated(subset=['text'], keep='first').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through some initial scrubbing of the dataset it is time to explore some of the characteristics of the tweet data. During this EDA phase, we will inspect the class balance, distribution of tweet lengths, WordClouds and most common words for each class. \n",
    "\n",
    "Because we are working with Twitter data, we'll work with nltk's TweetTokenizer and customize  stop words to get a better view of the content of the tweets for addressing the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.804705Z",
     "start_time": "2022-02-25T20:26:04.770466Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:04.831736Z",
     "start_time": "2022-02-25T20:26:04.813671Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_order = ('Negative', 'Positive',\n",
    "       'Neutral', \"Unknown\")\n",
    "\n",
    "sentiment_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:05.593778Z",
     "start_time": "2022-02-25T20:26:04.841917Z"
    }
   },
   "outputs": [],
   "source": [
    "## Overall sentiment distribution\n",
    "sns.catplot(data=df,x='sentiment',kind='count',order=sentiment_order,aspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examing the class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:05.636970Z",
     "start_time": "2022-02-25T20:26:05.604063Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:05.663863Z",
     "start_time": "2022-02-25T20:26:05.644866Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than half of the tweets were classified as having any emotion. Of the tweets which were tagged as having an emotion, most were coded positive. About 3,000 tweets compared to only 570 tweets that were tagged as having negative emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:05.705839Z",
     "start_time": "2022-02-25T20:26:05.671102Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['sentiment']=='Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tweets labeled as unknown are difficult to classify without more context and could be viewed as sarcastic.\n",
    "All tweets in the corpus will need to be classified for modeling later on and the volume accounts for less than 2% of the corpus it is safe to drop these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:05.746434Z",
     "start_time": "2022-02-25T20:26:05.719813Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['sentiment']!='Unknown']\n",
    "df['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the business problem we are looking to solve requires understanding differences between positive and negative sentiments, it is essential that posiitive and negative twwets are separated for the exploration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.940130Z",
     "start_time": "2022-02-25T20:26:05.760373Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "freq.plot(25);\n",
    "\n",
    "## Rotate \n",
    "ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                   rotation=45,ha='right');\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.962867Z",
     "start_time": "2022-02-25T20:25:56.475Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get the most_common 100 and make into a dataframe\n",
    "most_common = pd.DataFrame(freq.most_common(100),\n",
    "                           columns=['word','count']).sort_values('count',\n",
    "                                                                 ascending=True)\n",
    "most_common.set_index('word').tail(25).plot(kind='barh',figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.968691Z",
     "start_time": "2022-02-25T20:25:56.483Z"
    }
   },
   "outputs": [],
   "source": [
    "most_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.973259Z",
     "start_time": "2022-02-25T20:25:56.489Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_most_common(freq,n=25,figsize=(12,5)):\n",
    "    most_common = pd.DataFrame(freq.most_common(n),\n",
    "                           columns=['word','count']).sort_values('count',\n",
    "                                                                 ascending=True)\n",
    "    most_common.set_index('word').tail(n).plot(kind='barh',figsize=figsize)\n",
    "    \n",
    "plot_most_common(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.979938Z",
     "start_time": "2022-02-25T20:25:56.497Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a new freq dist for tweet tokens and plot most common\n",
    "tweet_freq = FreqDist(tweet_tokens)\n",
    "plot_most_common(tweet_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.983545Z",
     "start_time": "2022-02-25T20:25:56.509Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_df = df.loc[df['sentiment']=='Positive']\n",
    "positive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.987095Z",
     "start_time": "2022-02-25T20:25:56.522Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_df = df.loc[df['sentiment']=='Negative']\n",
    "negative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.992978Z",
     "start_time": "2022-02-25T20:25:56.537Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_corpus = positive_df['text'].to_list()\n",
    "positive_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:06.998044Z",
     "start_time": "2022-02-25T20:25:56.545Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_corpus = negative_df['text'].to_list()\n",
    "negative_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.006105Z",
     "start_time": "2022-02-25T20:25:56.551Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "import string\n",
    "\n",
    "#Function for tokenization of tweets\n",
    "def tweets_tokenize(corpus, preserve_case=False, strip_handles=True):\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=preserve_case, \n",
    "                               strip_handles=strip_handles)\n",
    "    tokens = tokenizer.tokenize(','.join(corpus))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.013364Z",
     "start_time": "2022-02-25T20:25:56.557Z"
    }
   },
   "outputs": [],
   "source": [
    "#positive tweets tokenized\n",
    "positive_tokens = tweets_tokenize(positive_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.017595Z",
     "start_time": "2022-02-25T20:25:56.565Z"
    }
   },
   "outputs": [],
   "source": [
    "#negative tweets tokenized\n",
    "negative_tokens = tweets_tokenize(negative_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.023207Z",
     "start_time": "2022-02-25T20:25:56.573Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the most common positive tokens\n",
    "from nltk import FreqDist\n",
    "freq = FreqDist(positive_tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.030235Z",
     "start_time": "2022-02-25T20:25:56.593Z"
    }
   },
   "outputs": [],
   "source": [
    "#checking the most common negative tokens\n",
    "from nltk import FreqDist\n",
    "freq = FreqDist(negative_tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are stop words and puncuations that were tokenized and will need to be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing StopWords, tokens should be lemmatized to ensure the list of words are being captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.036927Z",
     "start_time": "2022-02-25T20:25:56.608Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#Function for lemmatizating tokens\n",
    "def lemmatize_tokens(tokens_list):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_tokens = [lemmatizer.lemmatize(word) for word in tokens_list]\n",
    "    return lemma_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.045717Z",
     "start_time": "2022-02-25T20:25:56.617Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lemmatize positive tokens\n",
    "positive_tokens_lemma = lemmatize_tokens(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.051862Z",
     "start_time": "2022-02-25T20:25:56.622Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lemmatize negative tokens\n",
    "negative_tokens_lemma = lemmatize_tokens(negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation And StopWord Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.055548Z",
     "start_time": "2022-02-25T20:25:56.632Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import nltk's stopwords and add punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list += list(string.punctuation)\n",
    "#additional punctuation characters \n",
    "add_punct = ['“','”','...',\"''\",'’','``','']\n",
    "stopword_list += add_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.061083Z",
     "start_time": "2022-02-25T20:25:56.637Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to remove of StopWords\n",
    "def stopword_removal(tokens, stopword_list=stopword_list):\n",
    "    \n",
    "    #encoding tokens to remove unrecognized characters and url links\n",
    "    stopped_tokens = [w.encode('ascii','ignore').decode() for w in tokens \n",
    "                      if (w not in stopword_list) & \n",
    "                      (w.startswith('http') == False)]\n",
    "    \n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.064592Z",
     "start_time": "2022-02-25T20:25:56.644Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing StopWords from lemmatized tokens\n",
    "positive_lemma_stopped = stopword_removal(positive_tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.068811Z",
     "start_time": "2022-02-25T20:25:56.653Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing StopWords from lemmatized tokens\n",
    "negative_lemma_stopped = stopword_removal(negative_tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.073775Z",
     "start_time": "2022-02-25T20:25:56.660Z"
    }
   },
   "outputs": [],
   "source": [
    "#looking at the most common tokens\n",
    "freq = FreqDist(positive_lemma_stopped)\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.079368Z",
     "start_time": "2022-02-25T20:25:56.667Z"
    }
   },
   "outputs": [],
   "source": [
    "#looking at the most common tokens\n",
    "freq = FreqDist(negative_lemma_stopped)\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.085190Z",
     "start_time": "2022-02-25T20:25:56.676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending stopwords list\n",
    "stopword_list.extend(['rt','co','sxsw', '#sxsw', '#sxswi','link'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.088852Z",
     "start_time": "2022-02-25T20:25:56.686Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing StopWords from lemmatized tokens\n",
    "positive_lemma_stopped = stopword_removal(positive_tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.092764Z",
     "start_time": "2022-02-25T20:25:56.702Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing StopWords from lemmatized tokens\n",
    "negative_lemma_stopped = stopword_removal(negative_tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.099298Z",
     "start_time": "2022-02-25T20:25:56.716Z"
    }
   },
   "outputs": [],
   "source": [
    "#looking at the most common tokens\n",
    "freq = FreqDist(positive_lemma_stopped)\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.105265Z",
     "start_time": "2022-02-25T20:25:56.722Z"
    }
   },
   "outputs": [],
   "source": [
    "#looking at the most common tokens\n",
    "freq = FreqDist(negative_lemma_stopped)\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.109148Z",
     "start_time": "2022-02-25T20:25:56.731Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def wordcloud_generator(tokens, collocations=False, background_color='black', \n",
    "                       colormap='Greens', display=True):\n",
    "\n",
    "    \n",
    "    # Initalize a WordCloud\n",
    "    wordcloud = WordCloud(collocations=collocations, \n",
    "                          background_color=background_color, \n",
    "                          colormap=colormap, \n",
    "                          width=500, height=300)\n",
    "\n",
    "    # Generate wordcloud from tokens\n",
    "    wordcloud.generate(','.join(tokens))\n",
    "\n",
    "    # Plot with matplotlib\n",
    "    if display:\n",
    "        plt.figure(figsize = (12, 15), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis('off');\n",
    "        \n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.113527Z",
     "start_time": "2022-02-25T20:25:56.739Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate a WordCloud for positive tweets\n",
    "positive_cloud = wordcloud_generator(positive_lemma_stopped, collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.118119Z",
     "start_time": "2022-02-25T20:25:56.748Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate a WordCloud for negative tweets\n",
    "negative_cloud = wordcloud_generator(negative_lemma_stopped, colormap='Reds',\n",
    "                                     collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.124325Z",
     "start_time": "2022-02-25T20:25:56.754Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordcloud_comp(wc1, wc2):\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(30,20), ncols=2)\n",
    "    ax[0].imshow(wc1)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].imshow(wc2)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.129500Z",
     "start_time": "2022-02-25T20:25:56.762Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_comp(positive_cloud, negative_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the brands presenting new services and  and product launches at the event appeared most in both positive and negative tweets. Let's look at WordClouds with those words added to the stop list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.135133Z",
     "start_time": "2022-02-25T20:25:56.771Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing brands and products from the Wordcloud\n",
    "stopword_list_no_brands = stopword_list + ['ipad', 'ipad2','#ipad2','apple', 'google', 'iphone', \n",
    "                           '#apple','#google', '#ipad', '#iphone', 'android']\n",
    "\n",
    "positive_stopped_brands = stopword_removal(positive_tokens_lemma, stopword_list=stopword_list_no_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.139083Z",
     "start_time": "2022-02-25T20:25:56.784Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_stopped_brands = stopword_removal(negative_tokens_lemma, stopword_list=stopword_list_no_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.143887Z",
     "start_time": "2022-02-25T20:25:56.789Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_cloud_no_names = wordcloud_generator(positive_stopped_brands, collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.150289Z",
     "start_time": "2022-02-25T20:25:56.798Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_cloud_no_names = wordcloud_generator(negative_stopped_brands, \n",
    "                                              colormap='Reds',collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.158640Z",
     "start_time": "2022-02-25T20:25:56.804Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud_comp(positive_cloud_no_names,negative_cloud_no_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tweet Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.170192Z",
     "start_time": "2022-02-25T20:25:56.809Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.178795Z",
     "start_time": "2022-02-25T20:25:56.817Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_finder = BigramCollocationFinder.from_words(positive_lemma_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.188959Z",
     "start_time": "2022-02-25T20:25:56.823Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams = positive_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.192609Z",
     "start_time": "2022-02-25T20:25:56.830Z"
    }
   },
   "outputs": [],
   "source": [
    "#top 30 bigrams\n",
    "bigrams[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Sentiments of Products/Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.198699Z",
     "start_time": "2022-02-25T20:25:56.837Z"
    }
   },
   "outputs": [],
   "source": [
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.202209Z",
     "start_time": "2022-02-25T20:25:56.842Z"
    }
   },
   "outputs": [],
   "source": [
    "product_order = ['iPad','Apple','iPad or iPhone App','Google','iPhone',\n",
    "                 'Other Google product or service','Android App','Android',\n",
    "                 'Other Apple product or service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.207821Z",
     "start_time": "2022-02-25T20:25:56.848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Product tweet distribution\n",
    "sns.catplot(data=df,x='product',kind='count',order=product_order,aspect=3.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.212544Z",
     "start_time": "2022-02-25T20:25:56.854Z"
    }
   },
   "outputs": [],
   "source": [
    "#mapping products and services to their brand\n",
    "product_dict={'iPad': 'Apple', 'Apple': 'Apple', 'iPad or iPhone App': 'Apple', \n",
    "              'Google': 'Google', 'iPhone': 'Apple', \n",
    "              'Other Google product or service': 'Google',\n",
    "              'Android App': 'Google', 'Android': 'Google',\n",
    "              'Other Apple product or service': 'Apple'}\n",
    "\n",
    "df['brand'] = df['product'].map(product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.216797Z",
     "start_time": "2022-02-25T20:25:56.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Brand tweet distribution\n",
    "sns.catplot(data=df,x='brand',kind='count',order=['Apple', 'Google'],aspect=3.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.220457Z",
     "start_time": "2022-02-25T20:25:56.865Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['brand','product','sentiment']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.224709Z",
     "start_time": "2022-02-25T20:25:56.873Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    tokenizer_no_strip = TweetTokenizer(strip_handles=False)\n",
    "    tokens = tokenizer_no_strip.tokenize(doc)\n",
    "    return len(tokens)\n",
    "\n",
    "df['token_count'] = df['text'].map(lambda x: count_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.230741Z",
     "start_time": "2022-02-25T20:25:56.879Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize distribution of tweet lengths \n",
    "with sns.plotting_context(context='talk'):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.histplot(df['token_count'], color='lightblue', ax=ax)\n",
    "    ax.set_title('Distribution of Tweet Lengths')\n",
    "    ax.set_xlabel('Number of Tokens in Tweet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.235078Z",
     "start_time": "2022-02-25T20:25:56.886Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.238500Z",
     "start_time": "2022-02-25T20:25:56.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a DataFrame from the Bigrams\n",
    "pd.DataFrame(tweets_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.245926Z",
     "start_time": "2022-02-25T20:25:56.900Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "tweet_pmi_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweet_pmi_finder.apply_freq_filter(3)\n",
    "\n",
    "tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.250244Z",
     "start_time": "2022-02-25T20:25:56.907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a DataFrame from the Bigrams with PMI\n",
    "pd.DataFrame(tweet_pmi_scored,columns=['Words','PMI']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.254749Z",
     "start_time": "2022-02-25T20:25:56.915Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, plot_roc_curve, plot_confusion_matrix, roc_curve\n",
    "\n",
    "def clf_eval(y_true, y_pred, X_test, X_train, clf, n_class=3):\n",
    "    \n",
    "    \n",
    "    print(f\"Training Score: {round(clf.score(X_train, y_train),2)} \\\n",
    "            Test Score:{round(clf.score(X_test, y_true),2)}\")\n",
    "    \n",
    "   \n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(classification_report(y_true=y_true, y_pred=y_pred))\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "    \n",
    "    plot_confusion_matrix(estimator=clf, X=X_test, y_true=y_true, cmap='Blues', \n",
    "                          normalize='true', ax=ax[0], \n",
    "                          display_labels=['Negative','Neutral', 'Positive'])\n",
    " \n",
    "    pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "    try:\n",
    "        roc_curve = metrics.plot_roc_curve(clf,X_test_tf,y_test,ax=ax[1])\n",
    "        curve.ax_.grid()\n",
    "        curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "        fig.tight_layout()\n",
    "    except:\n",
    "        fig.delaxes(ax[1])\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.260071Z",
     "start_time": "2022-02-25T20:25:56.937Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['sentiment']\n",
    "X = df['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model will be used to measure how well our model performs compared to random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.265211Z",
     "start_time": "2022-02-25T20:25:56.962Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.269826Z",
     "start_time": "2022-02-25T20:25:56.978Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf_pipe = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                                                    stop_words=stopword_list)),\n",
    "                     ('clf', DummyClassifier(random_state=42))])\n",
    "\n",
    "clf_pipe.fit(X_train, y_train)\n",
    "y_pred = clf_pipe.predict(X_test)\n",
    "clf_eval(y_test, y_pred, X_test, X_train, clf_pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.273442Z",
     "start_time": "2022-02-25T20:25:56.999Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_text_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                                   stop_words=stopword_list)),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.277403Z",
     "start_time": "2022-02-25T20:25:57.021Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_text_pipe.fit(X_train, y_train)\n",
    "y_pred = mnb_text_pipe.predict(X_test)\n",
    "clf_eval(y_test, y_pred, X_test, X_train, mnb_text_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.281066Z",
     "start_time": "2022-02-25T20:25:57.058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_text_pipe = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                                   stop_words=stopword_list)), \n",
    "    ('clf', LogisticRegressionCV(solver='saga',max_iter=500, class_weight='balanced', random_state=42,n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.284496Z",
     "start_time": "2022-02-25T20:25:57.072Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_text_pipe.fit(X_train, y_train)\n",
    "y_pred = lr_text_pipe.predict(X_test)\n",
    "clf_eval(y_test, y_pred, X_test, X_train, lr_text_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.288709Z",
     "start_time": "2022-02-25T20:25:57.084Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'clf__class_weight': ['balanced'],\n",
    "              'clf__max_iter': [100, 500, 1000],\n",
    "              'clf__Cs': [[0.01], [0.1], [1]],\n",
    "              'clf__solver': ['liblinear', 'lbfgs', 'sag', 'saga']}\n",
    "\n",
    "gs = GridSearchCV(estimator=lr_text_pipe, param_grid = param_grid, \n",
    "                              scoring='recall_macro')\n",
    "\n",
    "gs.fit(X_train,  y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.292845Z",
     "start_time": "2022-02-25T20:25:57.101Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuned_lr_text_pipe = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                                   stop_words=stopword_list)), \n",
    "    ('clf', LogisticRegressionCV(solver='liblinear',\n",
    "                                 max_iter=100, \n",
    "                                 class_weight='balanced',\n",
    "                                 Cs=1,\n",
    "                                 random_state=42,\n",
    "                                 n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.299451Z",
     "start_time": "2022-02-25T20:25:57.118Z"
    }
   },
   "outputs": [],
   "source": [
    "tuned_lr_text_pipe.fit(X_train, y_train)\n",
    "y_pred = tuned_lr_text_pipe.predict(X_test)\n",
    "clf_eval(y_test, y_pred, X_test, X_train, tuned_lr_text_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.304986Z",
     "start_time": "2022-02-25T20:25:57.137Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_text_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                                   stop_words=stopword_list)), \n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.308756Z",
     "start_time": "2022-02-25T20:25:57.149Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_text_pipe.fit(X_train, y_train)\n",
    "y_pred = rf_text_pipe.predict(X_test)\n",
    "clf_eval(y_test, y_pred, X_test, X_train, rf_text_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how well your work solves the stated business problem.\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "Questions to consider:\n",
    "* How do you interpret the results?\n",
    "* How well does your model fit your data? How much better is this than your baseline model?\n",
    "* How confident are you that your results would generalize beyond the data you have?\n",
    "* How confident are you that this model would benefit the business if put into use?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.313750Z",
     "start_time": "2022-02-25T20:25:57.179Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_importance(clf_pipe, n_features, title):\n",
    "   \n",
    "    feats = clf_pipe['vectorizer'].get_feature_names()\n",
    "    coefs = clf_pipe['clf'].coef_[0]\n",
    "    \n",
    "    importance_df = pd.DataFrame(feats, columns=['Word'])\n",
    "    importance_df['Importance'] = math.e**(abs(coefs))\n",
    "    importance_df['Coefficient'] = coefs\n",
    "\n",
    "    feat_importance = importance_df.sort_values(by = [\"Importance\"], \n",
    "                                                   ascending=False).head(n_features)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15,10), ncols=2)\n",
    "    ax[0].set_title(f'Coefficients for {title}')\n",
    "    ax[0].set_ylabel('Word')\n",
    "    ax[0].set_xlabel('Coefficient')\n",
    "    sns.barplot(x='Coefficient', y='Word', palette='magma', data=feat_importance, ax=ax[0])\n",
    "\n",
    "    ax[1].set_title(f'Feature Importances for {title}')\n",
    "    ax[1].set_ylabel('Word')\n",
    "    ax[1].set_xlabel('Importance')\n",
    "    sns.barplot(x='Importance', y='Word', palette='magma', data=feat_importance, ax=ax[1])\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-25T20:26:07.317880Z",
     "start_time": "2022-02-25T20:25:57.186Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_importance(lr_text_pipe, 15, 'Tuned Logistic Regression Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
